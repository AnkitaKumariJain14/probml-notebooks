{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of mix_PPCA_celeba_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_vaqr1IEdWV"
      },
      "source": [
        "## Get the CelebA dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2KaFVMC5OsQ"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/sayantanauddy/vae_lightning/main/data.py "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvyDg8146HIX"
      },
      "source": [
        "%%capture \n",
        "! pip install torchvision pytorch-lightning torchmetrics  torch test-tube lightning-bolts umap-learn einops"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulw6CUhzKNRH"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91H5S1c0EkN6"
      },
      "source": [
        "## Get the Kaggle api token and upload it to colab. Follow the instructions [here](https://github.com/Kaggle/kaggle-api#api-credentials)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbKcXwCedIN8",
        "outputId": "7ab40940-bc62-4658-88e1-00394243c682"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "qncitxB3oyVF",
        "outputId": "fba82e5b-e5ae-424a-b4a3-1c7a13be2e9b"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-278a9c3d-9cbe-422b-a553-1a3ce280b389\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-278a9c3d-9cbe-422b-a553-1a3ce280b389\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjc9kbpaoyki"
      },
      "source": [
        "!mkdir /root/.kaggle"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3hCPht5pjwa"
      },
      "source": [
        "!cp kaggle.json /root/.kaggle/kaggle.json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81GmoSPCpj4T"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpW1offQ-eRh"
      },
      "source": [
        "## Getting the checkpoint of the model from buckets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2vQduXW7K__"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g43d9dfRxTif"
      },
      "source": [
        "bucket_name = 'probml_data' "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV0nGz0jKxIt"
      },
      "source": [
        "!mkdir /content/models "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Evj8R-wKxSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135782a1-86c6-473d-d7d1-d47bfa2bf484"
      },
      "source": [
        "!gsutil cp -r gs://{bucket_name}/mix_PPCA /content/models/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://probml_data/mix_PPCA/model_c_300_l_10_init_rnd_samples.pth...\n",
            "| [1 files][168.8 MiB/168.8 MiB]                                                \n",
            "Operation completed over 1 objects/168.8 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFlU3Ns6lHuy"
      },
      "source": [
        "# MFA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdbwCDTGkfw7"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import time\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "class MFA(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A class representing a Mixture of Factor Analyzers [1] / Mixture of Probabilistic PCA [2] in pytorch.\n",
        "    MFA/MPPCA are Gaussian Mixture Models with low-rank-plus-diagonal covariance, enabling efficient modeling\n",
        "    of high-dimensional domains in which the data resoides near lower-dimensional subspaces.\n",
        "    The implementation is based on [3] (please quote this if you are using this package in your research).\n",
        "    Attributes (model parameters):\n",
        "    ------------------------------\n",
        "    MU: Tensor shaped [n_components, n_features]\n",
        "        The component means.\n",
        "    A: Tensor shaped [n_components, n_features, n_factors]\n",
        "        The component subspace directions / factor loadings. These should be orthogonal (but not orthonormal)\n",
        "    lod_D: Tensor shaped [n_components, n_features]\n",
        "        Log of the component diagonal variance values. Note that in MPPCA, all values along the diagonal are the same.\n",
        "    PI_logits: Tensor shaped [n_components]\n",
        "        Log of the component mixing-coefficients (probabilities). Apply softmax to get the actual PI values.\n",
        "    Main Methods:\n",
        "    -------------\n",
        "    fit:\n",
        "        Fit the MPPCA model parameters to pre-loaded training data using EM\n",
        "    batch_fit:\n",
        "        Fit the MPPCA model parameters to a (possibly large) pytorch dataset using EM in batches\n",
        "    sample:\n",
        "        Generate new samples from the trained model\n",
        "    per_component_log_likelihood, log_prob, log_likelihood:\n",
        "        Probability query methods\n",
        "    responsibilities, log_responsibilities, map_component:\n",
        "        Responsibility (which component the sample comes from) query methods\n",
        "    reconstruct, conditional_reconstruct:\n",
        "        Reconstruction and in-painting\n",
        "    [1] Tipping, Michael E., and Christopher M. Bishop. \"Mixtures of probabilistic principal component analyzers.\"\n",
        "        Neural computation 11.2 (1999): 443-482.\n",
        "    [2] Ghahramani, Zoubin, and Geoffrey E. Hinton. \"The EM algorithm for mixtures of factor analyzers.\"\n",
        "        Vol. 60. Technical Report CRG-TR-96-1, University of Toronto, 1996.\n",
        "    [3] Richardson, Eitan, and Yair Weiss. \"On gans and gmms.\"\n",
        "        Advances in Neural Information Processing Systems. 2018.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_components, n_features, n_factors, isotropic_noise=True, init_method='rnd_samples'):\n",
        "        super(MFA, self).__init__()\n",
        "        self.n_components = n_components\n",
        "        self.n_features = n_features\n",
        "        self.n_factors = n_factors\n",
        "        self.init_method = init_method\n",
        "        self.isotropic_noise = isotropic_noise\n",
        "\n",
        "        self.MU = torch.nn.Parameter(torch.zeros(n_components, n_features), requires_grad=False)\n",
        "        self.A = torch.nn.Parameter(torch.zeros(n_components, n_features, n_factors), requires_grad=False)\n",
        "        self.log_D = torch.nn.Parameter(torch.zeros(n_components, n_features), requires_grad=False)\n",
        "        self.PI_logits = torch.nn.Parameter(torch.log(torch.ones(n_components)/float(n_components)), requires_grad=False)\n",
        "\n",
        "    def sample(self, n, with_noise=False):\n",
        "        \"\"\"\n",
        "        Generate random samples from the trained MFA / MPPCA\n",
        "        :param n: How many samples\n",
        "        :param with_noise: Add the isotropic / diagonal noise to the generated samples\n",
        "        :return: samples [n, n_features], c_nums - originating component numbers\n",
        "        \"\"\"\n",
        "        if torch.all(self.A == 0.):\n",
        "            warnings.warn('SGD MFA training requires initialization. Please call batch_fit() first.')\n",
        "\n",
        "        K, d, l = self.A.shape\n",
        "        c_nums = np.random.choice(K, n, p=torch.softmax(self.PI_logits, dim=0).detach().cpu().numpy())\n",
        "        z_l = torch.randn(n, l, device=self.A.device)\n",
        "        z_d = torch.randn(n, d, device=self.A.device) if with_noise else torch.zeros(n, d, device=self.A.device)\n",
        "        samples = torch.stack([self.A[c_nums[i]] @ z_l[i] + self.MU[c_nums[i]] + z_d[i] * torch.exp(0.5*self.log_D[c_nums[i]])\n",
        "                               for i in range(n)])\n",
        "        return samples, c_nums\n",
        "\n",
        "    @staticmethod\n",
        "    def _component_log_likelihood(x, PI, MU, A, log_D):\n",
        "        K, d, l = A.shape\n",
        "        AT = A.transpose(1, 2)\n",
        "        iD = torch.exp(-log_D).view(K, d, 1)\n",
        "        L = torch.eye(l, device=A.device).reshape(1, l, l) + AT @ (iD*A)\n",
        "        iL = torch.inverse(L)\n",
        "\n",
        "        def per_component_md(i):\n",
        "            x_c = (x - MU[i].reshape(1, d)).T  # shape = (d, n)\n",
        "            m_d_1 = (iD[i] * x_c) - ((iD[i] * A[i]) @ iL[i]) @ (AT[i] @ (iD[i] * x_c))\n",
        "            return torch.sum(x_c * m_d_1, dim=0)\n",
        "\n",
        "        m_d = torch.stack([per_component_md(i) for i in range(K)])\n",
        "        det_L = torch.logdet(L)\n",
        "        log_det_Sigma = det_L - torch.sum(torch.log(iD.reshape(K, d)), axis=1)\n",
        "        log_prob_data_given_components = -0.5 * ((d*np.log(2.0*math.pi) + log_det_Sigma).reshape(K, 1) + m_d)\n",
        "        return PI.reshape(1, K) + log_prob_data_given_components.T\n",
        "\n",
        "    def per_component_log_likelihood(self, x, sampled_features=None):\n",
        "        \"\"\"\n",
        "        Calculate per-sample and per-component log-likelihood values\n",
        "        :param x: samples [n, n_features]\n",
        "        :param sampled_features: list of feature coordinates to use\n",
        "        :return: log-probability values [n, n_components]\n",
        "        \"\"\"\n",
        "        if sampled_features is not None:\n",
        "            return MFA._component_log_likelihood(x[:, sampled_features], torch.softmax(self.PI_logits, dim=0),\n",
        "                                                 self.MU[:, sampled_features],\n",
        "                                                 self.A[:, sampled_features],\n",
        "                                                 self.log_D[:, sampled_features])\n",
        "        return MFA._component_log_likelihood(x, torch.softmax(self.PI_logits, dim=0), self.MU, self.A, self.log_D)\n",
        "\n",
        "    def log_prob(self, x, sampled_features=None):\n",
        "        \"\"\"\n",
        "        Calculate per-sample log-probability values\n",
        "        :param x: samples [n, n_features]\n",
        "        :param sampled_features: list of feature coordinates to use\n",
        "        :return: log-probability values [n]\n",
        "        \"\"\"\n",
        "        return torch.logsumexp(self.per_component_log_likelihood(x, sampled_features), dim=1)\n",
        "\n",
        "    def log_likelihood(self, x, sampled_features=None):\n",
        "        \"\"\"\n",
        "        Calculate the log-likelihood of the given data\n",
        "        :param x: samples [n, n_features]\n",
        "        :param sampled_features: list of feature coordinates to use\n",
        "        :return: (total) log-likelihood value\n",
        "        \"\"\"\n",
        "        return torch.sum(self.log_prob(x, sampled_features))\n",
        "\n",
        "    def log_responsibilities(self, x, sampled_features=None):\n",
        "        \"\"\"\n",
        "        Calculate the log-responsibilities (log of the responsibility values - probability of each sample to originate\n",
        "        from each of the component.\n",
        "        :param x: samples [n, n_features]\n",
        "        :param sampled_features: list of feature coordinates to use\n",
        "        :return: log-responsibilities values [n, n_components]\n",
        "        \"\"\"\n",
        "        comp_LLs = self.per_component_log_likelihood(x, sampled_features)\n",
        "        return comp_LLs - torch.logsumexp(comp_LLs, dim=1).reshape(-1, 1)\n",
        "\n",
        "    def responsibilities(self, x, sampled_features=None):\n",
        "        \"\"\"\n",
        "        Calculate the responsibilities - probability of each sample to originate from each of the component.\n",
        "        :param x: samples [n, n_features]\n",
        "        :param sampled_features: list of feature coordinates to use\n",
        "        :return: responsibility values [n, n_components]\n",
        "        \"\"\"\n",
        "        return torch.exp(self.log_responsibilities(x, sampled_features))\n",
        "\n",
        "    def map_component(self, x, sampled_features=None):\n",
        "        \"\"\"\n",
        "        Get the Maximum a Posteriori component numbers\n",
        "        :param x: samples [n, n_features]\n",
        "        :param sampled_features: list of feature coordinates to use\n",
        "        :return: component numbers [n]\n",
        "        \"\"\"\n",
        "        return torch.argmax(self.log_responsibilities(x, sampled_features), dim=1)\n",
        "\n",
        "    def conditional_reconstruct(self, full_x, observed_features):\n",
        "        \"\"\"\n",
        "        Calculates the mean of the conditional probability P(x_h | x_o)\n",
        "        References:\n",
        "        https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf#subsubsection.8.1.3\n",
        "        https://en.wikipedia.org/wiki/Woodbury_matrix_identity\n",
        "        Note: This is equivalent to calling reconstruct with sampled_features\n",
        "        :param full_x: the full vectors (including the hidden coordinates, which can contain any values)\n",
        "        :param observed_features: tensor containing a list of the observed coordinates of x\n",
        "        :return: A cloned version of full_x with the hidden features reconstructed\n",
        "        \"\"\"\n",
        "        assert observed_features is not None\n",
        "        K, d, l = self.A.shape\n",
        "        c_i = self.map_component(full_x, observed_features)\n",
        "\n",
        "        mask = torch.zeros(d, dtype=bool)\n",
        "        mask[observed_features] = True\n",
        "\n",
        "        A_a = self.A[c_i][:, ~mask, :]\n",
        "        A_b = self.A[c_i][:, mask, :]\n",
        "        MU_a = self.MU[c_i][:, ~mask]\n",
        "        MU_b = self.MU[c_i][:, mask]\n",
        "        iD_b = torch.exp(-self.log_D[c_i][:, mask]).unsqueeze(2)\n",
        "\n",
        "        iL_b = torch.inverse(torch.eye(l, device=MU_b.device).reshape(1, l, l) + A_b.transpose(1, 2) @ (iD_b*A_b))\n",
        "        x_b_l = ((A_b * iD_b).transpose(1,2) @ (full_x[:, mask] - MU_b).unsqueeze(2))\n",
        "        x_hat = full_x.clone()\n",
        "        x_hat[:, ~mask] =  (MU_a.unsqueeze(2) + A_a @ x_b_l - A_a @ (A_b.transpose(1, 2) @\n",
        "                                                                     (iD_b * (A_b @ iL_b @ x_b_l)))).squeeze(dim=2)\n",
        "        return x_hat\n",
        "\n",
        "    def reconstruct(self, full_x, observed_features=None):\n",
        "        \"\"\"\n",
        "        Reconstruct samples from the model - find the MAP component and latent z for each sample and regenerate\n",
        "        :param full_x: the full vectors (including the hidden coordinates, which can contain any values)\n",
        "        :param observed_features: tensor containing a list of the observed coordinates of x\n",
        "        :return: Reconstruction of full_x based on the trained model and observed features\n",
        "        \"\"\"\n",
        "        K, d, l = self.A.shape\n",
        "        c_i = self.map_component(full_x, observed_features)\n",
        "\n",
        "        used_features = observed_features if observed_features is not None else torch.arange(0, d)\n",
        "        x = full_x[:, used_features]\n",
        "        MU = self.MU[:, used_features]\n",
        "        A = self.A[:, used_features]\n",
        "        AT = A.transpose(1, 2)\n",
        "        iD = torch.exp(-self.log_D[:, used_features]).unsqueeze(2)\n",
        "        L = torch.eye(l, device=MU.device).reshape(1, l, l) + AT @ (iD*A)\n",
        "        iL = torch.inverse(L)\n",
        "\n",
        "        # per eq. 2 in Ghahramani and Hinton 1996 + the matrix inversion lemma (also described there).\n",
        "        x_c = (x - MU[c_i]).unsqueeze(2)\n",
        "        iD_c = iD[c_i]\n",
        "        m_d_1 = (iD_c * x_c) - ((iD_c * A[c_i]) @ iL[c_i]) @ (AT[c_i] @ (iD_c * x_c))\n",
        "        mu_z = AT[c_i] @ m_d_1\n",
        "        return (self.A[c_i] @ mu_z).reshape(-1, d) + self.MU[c_i]\n",
        "\n",
        "    @staticmethod\n",
        "    def _small_sample_ppca(x, n_factors):\n",
        "        # See https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\n",
        "        mu = torch.mean(x, dim=0)\n",
        "        # U, S, V = torch.svd(x - mu.reshape(1, -1))    # torch svd is less memory-efficient\n",
        "        U, S, V = np.linalg.svd((x - mu.reshape(1, -1)).cpu().numpy(), full_matrices=False)\n",
        "        V = torch.from_numpy(V.T).to(x.device)\n",
        "        S = torch.from_numpy(S).to(x.device)\n",
        "        sigma_squared = torch.sum(torch.pow(S[n_factors:], 2.0))/((x.shape[0]-1) * (x.shape[1]-n_factors))\n",
        "        A = V[:, :n_factors] * torch.sqrt((torch.pow(S[:n_factors], 2.0).reshape(1, n_factors)/(x.shape[0]-1) - sigma_squared))\n",
        "        return mu, A, torch.log(sigma_squared) * torch.ones(x.shape[1], device=x.device)\n",
        "\n",
        "    def _init_from_data(self, x, samples_per_component, feature_sampling=False):\n",
        "        n = x.shape[0]\n",
        "        K, d, l = self.A.shape\n",
        "\n",
        "        if self.init_method == 'kmeans':\n",
        "            # Import this only if 'kmeans' method was selected (not sure this is a good practice...)\n",
        "            from sklearn.cluster import KMeans\n",
        "            sampled_features = np.random.choice(d, int(d*feature_sampling)) if feature_sampling else np.arange(d)\n",
        "\n",
        "            t = time.time()\n",
        "            print('Performing K-means clustering of {} samples in dimension {} to {} clusters...'.format(\n",
        "                x.shape[0], sampled_features.size, K))\n",
        "            _x = x[:, sampled_features].cpu().numpy()\n",
        "            clusters = KMeans(n_clusters=K, max_iter=300, n_jobs=-1).fit(_x)\n",
        "            print('... took {} sec'.format(time.time() - t))\n",
        "            component_samples = [clusters.labels_ == i for i in range(K)]\n",
        "        elif self.init_method == 'rnd_samples':\n",
        "            m = samples_per_component\n",
        "            o = np.random.choice(n, m*K, replace=False) if m*K < n else np.arange(n)\n",
        "            assert n >= m*K\n",
        "            component_samples = [[o[i*m:(i+1)*m]] for i in range(K)]\n",
        "\n",
        "        params = [torch.stack(t) for t in zip(\n",
        "            *[MFA._small_sample_ppca(x[component_samples[i]], n_factors=l) for i in range(K)])]\n",
        "\n",
        "        self.MU.data = params[0]\n",
        "        self.A.data = params[1]\n",
        "        self.log_D.data = params[2]\n",
        "\n",
        "    def _parameters_sanity_check(self):\n",
        "        K, d, l = self.A.shape\n",
        "        assert torch.all(torch.softmax(self.PI_logits, dim=0) > 0.01/K), self.PI_logits\n",
        "        assert torch.all(torch.exp(self.log_D) > 1e-5) and torch.all(torch.exp(self.log_D) < 1.0), \\\n",
        "            '{} - {}'.format(torch.min(self.log_D).item(), torch.max(self.log_D).item())\n",
        "        assert torch.all(torch.abs(self.A) < 10.0), torch.max(torch.abs(self.A))\n",
        "        assert torch.all(torch.abs(self.MU) < 1.0), torch.max(torch.abs(self.MU))\n",
        "\n",
        "    def fit(self, x, max_iterations=20, feature_sampling=False):\n",
        "        \"\"\"\n",
        "        Estimate Maximum Likelihood MPPCA parameters for the provided data using EM per\n",
        "        Tipping, and Bishop. Mixtures of probabilistic principal component analyzers.\n",
        "        :param x: training data (arranged in rows), shape = (<numbr of samples>, n_features)\n",
        "        :param max_iterations: number of iterations\n",
        "        :param feature_sampling: allows faster responsibility calculation by sampling data coordinates\n",
        "        \"\"\"\n",
        "        assert self.isotropic_noise, 'EM fitting is currently supported for isotropic noise (MPPCA) only'\n",
        "        assert not feature_sampling or type(feature_sampling) == float, 'set to desired sampling ratio'\n",
        "        K, d, l = self.A.shape\n",
        "        N = x.shape[0]\n",
        "\n",
        "        print('Random init...')\n",
        "        init_samples_per_component = (l+1)*2 if self.init_method == 'rnd_samples' else (l+1)*10\n",
        "        self._init_from_data(x, samples_per_component=init_samples_per_component,\n",
        "                             feature_sampling=feature_sampling/2 if feature_sampling else False)\n",
        "        print('Init log-likelihood =', round(torch.mean(self.log_prob(x)).item(), 1))\n",
        "\n",
        "        def per_component_m_step(i):\n",
        "            mu_i = torch.sum(r[:, [i]] * x, dim=0) / r_sum[i]\n",
        "            s2_I = torch.exp(self.log_D[i, 0]) * torch.eye(l, device=x.device)\n",
        "            inv_M_i = torch.inverse(self.A[i].T @ self.A[i] + s2_I)\n",
        "            x_c = x - mu_i.reshape(1, d)\n",
        "            SiAi = (1.0/r_sum[i]) * (r[:, [i]]*x_c).T @ (x_c @ self.A[i])\n",
        "            invM_AT_Si_Ai = inv_M_i @ self.A[i].T @ SiAi\n",
        "            A_i_new = SiAi @ torch.inverse(s2_I + invM_AT_Si_Ai)\n",
        "            t1 = torch.trace(A_i_new.T @ (SiAi @ inv_M_i))\n",
        "            trace_S_i = torch.sum(N/r_sum[i] * torch.mean(r[:, [i]]*x_c*x_c, dim=0))\n",
        "            sigma_2_new = (trace_S_i - t1)/d\n",
        "            return mu_i, A_i_new, torch.log(sigma_2_new) * torch.ones_like(self.log_D[i])\n",
        "\n",
        "        for it in range(max_iterations):\n",
        "            t = time.time()\n",
        "            sampled_features = np.random.choice(d, int(d*feature_sampling)) if feature_sampling else None\n",
        "            r = self.responsibilities(x, sampled_features=sampled_features)\n",
        "            r_sum = torch.sum(r, dim=0)\n",
        "            new_params = [torch.stack(t) for t in zip(*[per_component_m_step(i) for i in range(K)])]\n",
        "            self.MU.data = new_params[0]\n",
        "            self.A.data = new_params[1]\n",
        "            self.log_D.data = new_params[2]\n",
        "            self.PI_logits.data = torch.log(r_sum / torch.sum(r_sum))\n",
        "            ll = round(torch.mean(self.log_prob(x)).item(), 1) if it % 5 == 0 else '.....'\n",
        "            print('Iteration {}/{}, train log-likelihood = {}, took {:.1f} sec'.format(it, max_iterations, ll,\n",
        "                                                                                   time.time()-t))\n",
        "\n",
        "    def batch_fit(self, train_dataset, test_dataset=None, batch_size=1000, test_size=1000, max_iterations=20,\n",
        "                  feature_sampling=False):\n",
        "        \"\"\"\n",
        "        Estimate Maximum Likelihood MPPCA parameters for the provided data using EM per\n",
        "        Tipping, and Bishop. Mixtures of probabilistic principal component analyzers.\n",
        "        Memory-efficient batched implementation for large datasets that do not fit in memory:\n",
        "        E step:\n",
        "            For all mini-batches:\n",
        "            - Calculate and store responsibilities\n",
        "            - Accumulate sufficient statistics\n",
        "        M step: \n",
        "            Re-calculate all parameters\n",
        "        Note that incremental EM per Neal & Hinton, 1998 is not supported, since we can't maintain\n",
        "            the full x x^T as sufficient statistic - we need to multiply by A to get a more compact\n",
        "            representation.\n",
        "        :param train_dataset: pytorch Dataset object containing the training data (will be iterated over)\n",
        "        :param test_dataset: optional pytorch Dataset object containing the test data (otherwise train_daset will be used)\n",
        "        :param batch_size: the batch size\n",
        "        :param test_size: number of samples to use when reporting likelihood\n",
        "        :param max_iterations: number of iterations (=epochs)\n",
        "        :param feature_sampling: allows faster responsibility calculation by sampling data coordinates\n",
        "       \"\"\"\n",
        "        assert self.isotropic_noise, 'EM fitting is currently supported for isotropic noise (MPPCA) only'\n",
        "        assert not feature_sampling or type(feature_sampling) == float, 'set to desired sampling ratio'\n",
        "        K, d, l = self.A.shape\n",
        "\n",
        "        init_samples_per_component = (l+1)*2 if self.init_method == 'rnd_samples' else (l+1)*10\n",
        "        print('Random init using {} with {} samples per component...'.format(self.init_method, init_samples_per_component))\n",
        "        init_keys = [key for i, key in enumerate(RandomSampler(train_dataset)) if i < init_samples_per_component*K]\n",
        "        init_samples, _ = zip(*[train_dataset[key] for key in init_keys])\n",
        "        self._init_from_data(torch.stack(init_samples).to(self.MU.device),\n",
        "                             samples_per_component=init_samples_per_component,\n",
        "                             feature_sampling=feature_sampling/2 if feature_sampling else False)\n",
        "\n",
        "        # Read some test samples for test likelihood calculation\n",
        "        # test_samples, _ = zip(*[test_dataset[key] for key in RandomSampler(test_dataset, num_samples=test_size, replacement=True)])\n",
        "        test_dataset = test_dataset or train_dataset\n",
        "        all_test_keys = [key for key in SequentialSampler(test_dataset)]\n",
        "        test_samples, _ = zip(*[test_dataset[key] for key in all_test_keys[:test_size]])\n",
        "        test_samples = torch.stack(test_samples).to(self.MU.device)\n",
        "\n",
        "        ll_log = []\n",
        "        loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "        for it in range(max_iterations):\n",
        "            t = time.time()\n",
        "\n",
        "            # Sufficient statistics\n",
        "            sum_r = torch.zeros(size=[K], dtype=torch.float64, device=self.MU.device)\n",
        "            sum_r_x = torch.zeros(size=[K, d], dtype=torch.float64, device=self.MU.device)\n",
        "            sum_r_x_x_A = torch.zeros(size=[K, d, l], dtype=torch.float64, device=self.MU.device)\n",
        "            sum_r_norm_x = torch.zeros(K, dtype=torch.float64, device=self.MU.device)\n",
        "\n",
        "            ll_log.append(torch.mean(self.log_prob(test_samples)).item())\n",
        "            print('Iteration {}/{}, log-likelihood={}:'.format(it, max_iterations, ll_log[-1]))\n",
        "\n",
        "            for batch_x, _ in loader:\n",
        "                print('E', end='', flush=True)\n",
        "                batch_x = batch_x.to(self.MU.device)\n",
        "                sampled_features = np.random.choice(d, int(d*feature_sampling)) if feature_sampling else None\n",
        "                batch_r = self.responsibilities(batch_x, sampled_features=sampled_features)\n",
        "                sum_r += torch.sum(batch_r, dim=0).double()\n",
        "                sum_r_norm_x += torch.sum(batch_r * torch.sum(torch.pow(batch_x, 2.0), dim=1, keepdim=True), dim=0).double()\n",
        "                for i in range(K):\n",
        "                    batch_r_x = batch_r[:, [i]] * batch_x\n",
        "                    sum_r_x[i] += torch.sum(batch_r_x, dim=0).double()\n",
        "                    sum_r_x_x_A[i] += (batch_r_x.T @ (batch_x @ self.A[i])).double()\n",
        "\n",
        "            print(' / M...', end='', flush=True)\n",
        "            self.PI_logits.data = torch.log(sum_r / torch.sum(sum_r)).float()\n",
        "            self.MU.data = (sum_r_x / sum_r.reshape(-1, 1)).float()\n",
        "            SA = sum_r_x_x_A / sum_r.reshape(-1, 1, 1) - \\\n",
        "                 (self.MU.reshape(K, d, 1) @ (self.MU.reshape(K, 1, d) @ self.A)).double()\n",
        "            s2_I = torch.exp(self.log_D[:, 0]).reshape(K, 1, 1) * torch.eye(l, device=self.MU.device).reshape(1, l, l)\n",
        "            M = (self.A.transpose(1, 2) @ self.A + s2_I).double()\n",
        "            inv_M = torch.stack([torch.inverse(M[i]) for i in range(K)])   # (K, l, l)\n",
        "            invM_AT_S_A = inv_M @ self.A.double().transpose(1, 2) @ SA   # (K, l, l)\n",
        "            self.A.data = torch.stack([(SA[i] @ torch.inverse(s2_I[i].double() + invM_AT_S_A[i])).float()\n",
        "                                       for i in range(K)])\n",
        "            t1 = torch.stack([torch.trace(self.A[i].double().T @ (SA[i] @ inv_M[i])) for i in range(K)])\n",
        "            t_s = sum_r_norm_x / sum_r - torch.sum(torch.pow(self.MU, 2.0), dim=1).double()\n",
        "            self.log_D.data = torch.log((t_s - t1)/d).float().reshape(-1, 1) * torch.ones_like(self.log_D)\n",
        "\n",
        "            self._parameters_sanity_check()\n",
        "            print(' ({} sec)'.format(time.time()-t))\n",
        "\n",
        "        ll_log.append(torch.mean(self.log_prob(test_samples)).item())\n",
        "        print('\\nFinal train log-likelihood={}:'.format(ll_log[-1]))\n",
        "        return ll_log\n",
        "\n",
        "    def sgd_mfa_train(self, train_dataset, test_dataset=None, batch_size=128, test_size=1000, max_epochs=10,\n",
        "                      learning_rate=0.001, feature_sampling=False):\n",
        "        \"\"\"\n",
        "        Stochastic Gradient Descent training of MFA (after initialization using MPPCA EM)\n",
        "        :param train_dataset: pytorch Dataset object containing the training data (will be iterated over)\n",
        "        :param test_dataset: optional pytorch Dataset object containing the test data (otherwise train_daset will be used)\n",
        "        :param batch_size: the batch size\n",
        "        :param test_size: number of samples to use when reporting likelihood\n",
        "        :param max_epochs: number of epochs\n",
        "        :param feature_sampling: allows faster responsibility calculation by sampling data coordinates\n",
        "        \"\"\"\n",
        "        if torch.all(self.A == 0.):\n",
        "            warnings.warn('SGD MFA training requires initialization. Please call batch_fit() first.')\n",
        "        if self.isotropic_noise:\n",
        "            warnings.warn('Currently, SGD training uses diagonal (non-isotropic) noise covariance i.e. MFA and not MPPCA')\n",
        "        assert not feature_sampling or type(feature_sampling) == float, 'set to desired sampling ratio'\n",
        "        # self.PI_logits.requires_grad =\n",
        "        self.MU.requires_grad = self.A.requires_grad = self.log_D.requires_grad = True\n",
        "        K, d, l = self.A.shape\n",
        "\n",
        "        # Read some test samples for test likelihood calculation\n",
        "        # test_samples, _ = zip(*[test_dataset[key] for key in RandomSampler(test_dataset, num_samples=test_size, replacement=True)])\n",
        "        test_dataset = test_dataset or train_dataset\n",
        "        all_test_keys = [key for key in SequentialSampler(test_dataset)]\n",
        "        test_samples, _ = zip(*[test_dataset[key] for key in all_test_keys[:test_size]])\n",
        "        test_samples = torch.stack(test_samples).to(self.MU.device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "        ll_log = []\n",
        "        self.train()\n",
        "        for epoch in range(max_epochs):\n",
        "            t = time.time()\n",
        "            for idx, (batch_x, _) in enumerate(loader):\n",
        "                print('.', end='', flush=True)\n",
        "                if idx > 0 and idx%100 == 0:\n",
        "                    print(torch.mean(self.log_prob(test_samples)).item())\n",
        "                sampled_features = np.random.choice(d, int(d*feature_sampling)) if feature_sampling else None\n",
        "                batch_x = batch_x.to(self.MU.device)\n",
        "                optimizer.zero_grad()\n",
        "                loss = -self.log_likelihood(batch_x, sampled_features=sampled_features) / batch_size\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            ll_log.append(torch.mean(self.log_prob(test_samples)).item())\n",
        "            print('\\nEpoch {}: Test ll = {} ({} sec)'.format(epoch, ll_log[-1], time.time()-t))\n",
        "            self._parameters_sanity_check()\n",
        "        self.PI_logits.requires_grad = self.MU.requires_grad = self.A.requires_grad = self.log_D.requires_grad = False\n",
        "        return ll_log"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEVEoWM6lK8p"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uBW2zYbkgTg"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "#from mfa import MFA\n",
        "\n",
        "\n",
        "class ReshapeTransform:\n",
        "    def __init__(self, new_size):\n",
        "        self.new_size = new_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return torch.reshape(img, self.new_size)\n",
        "\n",
        "\n",
        "class CropTransform:\n",
        "    def __init__(self, bbox):\n",
        "        self.bbox = bbox\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return img.crop(self.bbox)\n",
        "\n",
        "def samples_to_np_images(samples, image_shape=[64, 64, 3], clamp=True):\n",
        "    assert len(samples.shape) == 2\n",
        "    assert samples.shape[1] == np.prod(image_shape)\n",
        "    assert len(image_shape) == 2 or (len(image_shape) == 3 and image_shape[2] > 1)\n",
        "    samples_out = samples if not clamp else torch.clamp(samples, 0., 1.)\n",
        "    if len(image_shape) == 3:\n",
        "        return samples_out.reshape(-1, image_shape[2], image_shape[0], image_shape[1]).permute(0, 2, 3, 1).cpu().numpy()\n",
        "    else:\n",
        "        return samples_out.reshape(-1, image_shape[0], image_shape[1]).cpu().numpy()\n",
        "\n",
        "\n",
        "def sample_to_np_image(sample, image_shape=[64, 64, 3]):\n",
        "    return samples_to_np_images(sample.unsqueeze(0), image_shape).squeeze()\n",
        "\n",
        "\n",
        "def samples_to_mosaic(samples, image_shape=[64, 64, 3]):\n",
        "    images = samples_to_np_images(samples, image_shape)\n",
        "    num_images = images.shape[0]\n",
        "    num_cols = int(np.ceil(np.sqrt(num_images)))\n",
        "    rows = []\n",
        "    for i in range(num_images // num_cols):\n",
        "        rows.append(np.hstack([images[j] for j in range(i*num_cols, (i+1)*num_cols)]))\n",
        "    return np.vstack(rows)\n",
        "\n",
        "\n",
        "def visualize_model(model: MFA, image_shape=[64, 64, 3], start_component=0, end_component=None):\n",
        "    assert len(image_shape) == 2 or (len(image_shape) == 3 and image_shape[2] > 1)\n",
        "    K, d, l = model.A.shape\n",
        "    h, w = image_shape[:2]\n",
        "    spacer = min(8, w//8)\n",
        "    end_component = end_component or min(K, 2048//(w*3+2+spacer))\n",
        "    k = end_component - start_component\n",
        "    z = 1.5\n",
        "\n",
        "    def to_im(x):\n",
        "        return sample_to_np_image(x, image_shape=image_shape)\n",
        "\n",
        "    if len(image_shape) == 3:\n",
        "        canvas = np.ones([(l+1)*(h+1), k*(w*3+2) + (k-1)*spacer, image_shape[2]])\n",
        "    else:\n",
        "        canvas = np.ones([(l+1)*(h+1), k*(w*3+2) + (k-1)*spacer])\n",
        "    for c_num in range(start_component, end_component):\n",
        "        x_start = (c_num-start_component)*(w*3+2+spacer)\n",
        "\n",
        "        mu = model.MU[c_num]\n",
        "        canvas[:h, x_start+w//2:x_start+w//2+w] = to_im(mu)\n",
        "\n",
        "        D = torch.exp(0.5*model.log_D[c_num])\n",
        "        canvas[:h, x_start+w//2+w+2:x_start+w//2+2*w+2] = to_im(D / torch.max(D))\n",
        "\n",
        "        for i in range(l):\n",
        "            y_start = (i+1)*(h+1)\n",
        "            A_i = model.A[c_num, :, i]\n",
        "            canvas[y_start:y_start+h, x_start:x_start+w] = to_im(mu + z * A_i)\n",
        "            canvas[y_start:y_start+h, x_start+w+1:x_start+2*w+1] = to_im(0.5 + z * A_i)\n",
        "            canvas[y_start:y_start+h, x_start+2*w+2:x_start+3*w+2] = to_im(mu - z * A_i)\n",
        "    return canvas"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWU2S_TslNHS"
      },
      "source": [
        "# Loading the checkpoints "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOIYhDs5kgYh",
        "outputId": "edbaa6fd-f83e-4995-fc73-083e477077c3"
      },
      "source": [
        "import sys, os\n",
        "import torch\n",
        "from torchvision.datasets import CelebA, MNIST\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from imageio import imwrite\n",
        "from packaging import version\n",
        "from data import CelebADataset,  CelebADataModule\n",
        "\n",
        "def main(argv):\n",
        "    assert version.parse(torch.__version__) >= version.parse('1.2.0')\n",
        "\n",
        "    dataset = argv[1] if len(argv) == 2 else 'celeba'\n",
        "    print('Preparing dataset and parameters for', dataset, '...')\n",
        "\n",
        "    if dataset == 'celeba':\n",
        "        image_shape = [64, 64, 3]       # The input image shape\n",
        "        n_components = 300              # Number of components in the mixture model\n",
        "        n_factors = 10                  # Number of factors - the latent dimension (same for all components)\n",
        "        batch_size = 1000               # The EM batch size\n",
        "        num_iterations = 1              # Number of EM iterations (=epochs)\n",
        "        feature_sampling = 0.2          # For faster responsibilities calculation, randomly sample the coordinates (or False)\n",
        "        mfa_sgd_epochs = 0              # Perform additional training with diagonal (per-pixel) covariance, using SGD\n",
        "        init_method = 'rnd_samples'     # Initialize each component from few random samples using PPCA\n",
        "        trans = transforms.Compose([CropTransform((25, 50, 25+128, 50+128)), transforms.Resize(image_shape[0]), transforms.ToTensor(),  ReshapeTransform([-1])])\n",
        "        train_set = CelebADataset(root='./data', split='train', transform=trans, download=True) \n",
        "        test_set = CelebADataset(root='./data', split='test', transform=trans, download=True) \n",
        "    elif dataset == 'mnist':\n",
        "        image_shape = [28, 28]          # The input image shape\n",
        "        n_components = 50               # Number of components in the mixture model\n",
        "        n_factors = 6                   # Number of factors - the latent dimension (same for all components)\n",
        "        batch_size = 1000               # The EM batch size\n",
        "        num_iterations = 1              # Number of EM iterations (=epochs)\n",
        "        feature_sampling = False        # For faster responsibilities calculation, randomly sample the coordinates (or False)\n",
        "        mfa_sgd_epochs = 0              # Perform additional training with diagonal (per-pixel) covariance, using SGD\n",
        "        init_method = 'kmeans'          # Initialize by using k-means clustering\n",
        "        trans = transforms.Compose([transforms.ToTensor(),  ReshapeTransform([-1])])\n",
        "        train_set = MNIST(root='./data', train=True, transform=trans, download=True)\n",
        "        test_set = MNIST(root='./data', train=False, transform=trans, download=True)\n",
        "    else:\n",
        "        assert False, 'Unknown dataset: ' + dataset\n",
        "\n",
        "    model_name = 'c_{}_l_{}_init_{}'.format(n_components, n_factors, init_method)\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model_dir = './models/' + 'mix_PPCA'  \n",
        "    figures_dir = './figures/' + dataset\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    print('Loading pre-trained MFA model...')\n",
        "    model = MFA(n_components=n_components, n_features=np.prod(image_shape), n_factors=n_factors).to(device=device)\n",
        "    model.load_state_dict(torch.load(os.path.join(model_dir, 'model_c_300_l_10_init_rnd_samples.pth'))) #os.path.join(model_dir, 'model_c_{}_l_{}.pth'.format(n_components, n_factors)\n",
        "\n",
        "    print('Visualizing the trained model...')\n",
        "    model_image = visualize_model(model, image_shape=image_shape, end_component=10)\n",
        "    imwrite(os.path.join(figures_dir, 'model_'+model_name+'.jpg'), model_image)\n",
        "\n",
        "    print('Generating random samples...')\n",
        "    rnd_samples, _ = model.sample(100, with_noise=False)\n",
        "    mosaic = samples_to_mosaic(rnd_samples, image_shape=image_shape)\n",
        "    imwrite(os.path.join(figures_dir, 'samples_'+model_name+'.jpg'), mosaic)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(sys.argv)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing dataset and parameters for celeba ...\n",
            "Downloading dataset. Please while while the download and extraction processes complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5.00M/1.33G [00:00<00:41, 34.3MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading celeba-dataset.zip to ./data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.33G/1.33G [00:12<00:00, 116MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.02M/2.02M [00:00<00:00, 151MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading list_attr_celeba.csv.zip to ./data\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1.54M/1.54M [00:00<00:00, 203MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading list_bbox_celeba.csv.zip to ./data\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 466k/466k [00:00<00:00, 119MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading list_eval_partition.csv.zip to ./data\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 2.07M/2.07M [00:00<00:00, 227MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading list_landmarks_align_celeba.csv.zip to ./data\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files exist already\n",
            "Loading pre-trained MFA model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Visualizing the trained model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating random samples...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yixzHO5DlSMk"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFnoD89Nkgf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc357f52-1f5f-432b-b9d1-8a70813d9877"
      },
      "source": [
        "import os\n",
        "from torchvision.datasets import CelebA\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "from imageio import imwrite\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\"\"\"\n",
        "Examples for inference using the trained MFA model - likelihood evaluation and (conditional) reconstruction\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = 'celeba'\n",
        "    find_outliers = True\n",
        "    reconstruction = True\n",
        "    inpainting = True\n",
        "\n",
        "    print('Preparing dataset and parameters for', dataset, '...')\n",
        "    if dataset == 'celeba':\n",
        "        image_shape = [64, 64, 3]       # The input image shape\n",
        "        n_components = 300              # Number of components in the mixture model\n",
        "        n_factors = 10                  # Number of factors - the latent dimension (same for all components)\n",
        "        batch_size = 128                # The EM batch size\n",
        "        num_iterations = 10             # Number of EM iterations (=epochs)\n",
        "        feature_sampling = 0.2          # For faster responsibilities calculation, randomly sample the coordinates (or False)\n",
        "        mfa_sgd_epochs = 0              # Perform additional training with diagonal (per-pixel) covariance, using SGD\n",
        "        trans = transforms.Compose([CropTransform((25, 50, 25+128, 50+128)), transforms.Resize(image_shape[0]),\n",
        "                                    transforms.ToTensor(),  ReshapeTransform([-1])])\n",
        "        test_dataset = CelebADataset(root='./data', split='test', transform=trans, download=True)\n",
        "        # The train set has more interesting outliers...\n",
        "        # test_dataset = CelebA(root='./data', split='train', transform=trans, download=True)\n",
        "    else:\n",
        "        assert False, 'Unknown dataset: ' + dataset\n",
        "\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model_dir = './models/' + 'mix_PPCA' \n",
        "    figures_dir = './figures/' + dataset\n",
        "    os.makedirs(figures_dir, exist_ok=True)\n",
        "\n",
        "    print('Loading pre-trained MFA model...')\n",
        "    model = MFA(n_components=n_components, n_features=np.prod(image_shape), n_factors=n_factors).to(device=device)\n",
        "    model.load_state_dict(torch.load(os.path.join(model_dir, 'model_c_300_l_10_init_rnd_samples.pth'))) \n",
        "\n",
        "    if find_outliers:\n",
        "        print('Finding dataset outliers...')\n",
        "        loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "        all_ll = []\n",
        "        for batch_x, _ in tqdm(loader):\n",
        "            all_ll.append(model.log_prob(batch_x.to(device)))\n",
        "        all_ll = torch.cat(all_ll, dim=0)\n",
        "        ll_sorted = torch.argsort(all_ll).cpu().numpy()\n",
        "\n",
        "        all_keys = [key for key in SequentialSampler(test_dataset)]\n",
        "        outlier_samples, _ = zip(*[test_dataset[all_keys[ll_sorted[i]]] for i in range(100)])\n",
        "        mosaic = samples_to_mosaic(torch.stack(outlier_samples), image_shape=image_shape)\n",
        "        imwrite(os.path.join(figures_dir, 'outliers.jpg'), mosaic)\n",
        "\n",
        "    if reconstruction:\n",
        "        print('Reconstructing images from the trained model...')\n",
        "        random_samples, _ = zip(*[test_dataset[k] for k in RandomSampler(test_dataset, replacement=True, num_samples=100)]) #num_samples=100\n",
        "        random_samples = torch.stack(random_samples)\n",
        "\n",
        "        print('Reconstructing images from the trained model but lesser no. ...')\n",
        "        random_samples_less, _ = zip(*[test_dataset[k] for k in RandomSampler(test_dataset, replacement=True, num_samples=9)]) #num_samples=100\n",
        "        random_samples_less = torch.stack(random_samples_less)\n",
        "\n",
        "        if inpainting:\n",
        "            # Hide part of each image\n",
        "            w = image_shape[0]\n",
        "            mask = np.ones([3, w, w], dtype=np.float32)\n",
        "            #mask[:, w//4:-w//4, w//4:-w//4] = 0\n",
        "            #mask[:, w//2:, :] = 0\n",
        "            #mask[:, :, w//2:] = 0\n",
        "            #mask[:, :, :w//2] = 0\n",
        "            mask[:, :w//2, :] = 0\n",
        "            \n",
        "            mask = torch.from_numpy(mask.flatten()).reshape([1, -1])\n",
        "            \n",
        "            random_samples *= mask\n",
        "            random_samples_less *= mask\n",
        "            used_features = torch.nonzero(mask.flatten()).flatten()\n",
        "            reconstructed_samples = model.conditional_reconstruct(random_samples.to(device), observed_features=used_features).cpu()\n",
        "            reconstructed_samples_less = model.conditional_reconstruct(random_samples_less.to(device), observed_features=used_features).cpu()\n",
        "        else:\n",
        "            reconstructed_samples = model.reconstruct(random_samples.to(device)).cpu()\n",
        "            reconstructed_samples_less = model.reconstruct(random_samples_less.to(device)).cpu()\n",
        "\n",
        "        if inpainting:\n",
        "            reconstructed_samples = random_samples * mask + reconstructed_samples * (1 - mask)\n",
        "            reconstructed_samples_less = random_samples_less * mask + reconstructed_samples_less * (1 - mask)\n",
        "\n",
        "        mosaic_original = samples_to_mosaic(random_samples, image_shape=image_shape)\n",
        "        imwrite(os.path.join(figures_dir, 'original_samples.jpg'), mosaic_original)\n",
        "        mosaic_recontructed = samples_to_mosaic(reconstructed_samples, image_shape=image_shape)\n",
        "        imwrite(os.path.join(figures_dir, 'reconstructed_samples.jpg'), mosaic_recontructed)\n",
        "\n",
        "        mosaic_original_less = samples_to_mosaic(random_samples_less, image_shape=image_shape)\n",
        "        imwrite(os.path.join(figures_dir, 'original_samples_less.jpg'), mosaic_original_less)\n",
        "        mosaic_recontructed_less = samples_to_mosaic(reconstructed_samples_less, image_shape=image_shape)\n",
        "        imwrite(os.path.join(figures_dir, 'reconstructed_samples_less.jpg'), mosaic_recontructed_less)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing dataset and parameters for celeba ...\n",
            "Files exist already\n",
            "Loading pre-trained MFA model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\r  0%|          | 0/156 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finding dataset outliers...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 156/156 [00:30<00:00,  5.18it/s]\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reconstructing images from the trained model...\n",
            "Reconstructing images from the trained model but lesser no. ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}